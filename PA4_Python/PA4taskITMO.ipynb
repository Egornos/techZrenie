{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Authors**\n",
    "\n",
    "*The work can be done in groups of up to 3 students. Please complete the following fields with your group number and list your names along with ISU ID numbers.*\n",
    "\n",
    "> **Technical Vision**\n",
    ">\n",
    "> 1. Name, ITMO id\n",
    "> \n",
    "\n",
    "*The task and guidelines were prepared by Andrei Zhdanov and Sergei Shavetov, ITMO University, 2025.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Practical Assignment No 4. Image Segmentation**\n",
    "\n",
    "***Study of the basic methods for image segmentation into semantic areas.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the current practical assignment task we would need OpenCV and NumPy libraries along with image display functions we wrote during the Image Processing class. We import OpenCV's `cv2` library as `cv` for easier use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OpenCV library both as cv and cv2\n",
    "import cv2\n",
    "import cv2 as cv\n",
    "# Import NumPy library as np\n",
    "import numpy as np\n",
    "# Import sys for floats epsilon value\n",
    "import sys\n",
    "# Import scikit-image for an entropy filter\n",
    "import skimage.filters.rank\n",
    "import skimage.morphology\n",
    "# Import functions from our utility library\n",
    "from pa_utils import ShowImages, exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "In the current practical assignment we will learn some basic segmentation methods. These will be various image intensity thresholding methods and color segmentation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Task 1. Binarization**\n",
    "\n",
    "***Take an arbitrary image. Perform the image binarization using the considered methods. Depending on the image, use upper or lower threshold binarization.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to segment an image into two classes (background and object pixels) is by *binarization*. In this case we will convert our image to black-and-white color model by applying threshold to a image pixel intensity values. There are several common approached how it can be done, so let's check them out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general image binarization algorithm would be as follows:\n",
    "\n",
    "1. Load an image.\n",
    "2. Convert it to grayscale.\n",
    "3. Threshold the grayscale image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.1 Simple Thresholding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try the thresholding algorithms on the classic Lena image. Let's read it in BGR and then convert to a GRAYSCALE mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read an image from a file in BGR\n",
    "fn = \"images/lena_color.png\"\n",
    "I1 = cv.imread(fn, cv.IMREAD_COLOR)\n",
    "if not isinstance(I1, np.ndarray) or I1.data == None:\n",
    "  print(\"Error reading file \\\"{}\\\"\".format(fn))\n",
    "  exit()\n",
    "\n",
    "# Convert loaded BGR image to grayscale\n",
    "I1gray = cv.cvtColor(I1, cv.COLOR_BGR2GRAY)\n",
    "# Display it\n",
    "ShowImages([(\"Source image\", I1), \n",
    "            (\"Grayscale\", I1gray)], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Single thresholding\n",
    "\n",
    "The first and the most simple binarization method is the single thresholding method which is described with the following formula:\n",
    "\n",
    "$$\n",
    "I_{st}(x,y) = \n",
    "\\begin{cases}\n",
    "\t0 & \\textit{if } I(x,y) \\leqslant t,  \\\\\n",
    "\t1 & \\textit{if } I(x,y)> t,\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where \n",
    "- $I$ is the source image;\n",
    "- $I_{st}$ is the single threshold binarized image;\n",
    "- $t$ is the binarization threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In OpenCV, the  thresholding operation is executed with `cv2.threshold(image, t, maxval, method)` function. It takes an `image` to process, the threshold parameter `t`, the value to set for thresholded image pixels `maxval`, and the thresholding `method` as its parameters. It returns a tuple with the threshold used during thresholding and the thresholded image.\n",
    "\n",
    "The following single thresholding methods are supported in OpenCV:\n",
    "- `cv2.THRESH_BINARY` - set everything greater than $t$ value to the $maxval$ and everything else to $0$:\n",
    "$$\n",
    "I_{binary}(x,y) = \n",
    "\\begin{cases}\n",
    "  0 & \\textit{if } I(x,y) \\leqslant t,  \\\\\n",
    "  maxval &  \\textit{if } I(x,y)> t,\n",
    "\\end{cases}\n",
    "$$\n",
    "- `cv2.THRESH_BINARY_INV` - set everything greater than $t$ value to the $0$ and everything else to $maxval$:\n",
    "$$\n",
    "I_{binary\\_inv}(x,y) = \n",
    "\\begin{cases}\n",
    "  maxval & \\textit{if } I(x,y) \\leqslant t,  \\\\\n",
    "  0 &  \\textit{if } I(x,y)> t,\n",
    "\\end{cases}\n",
    "$$\n",
    "- `cv2.THRESH_TRUNC` - truncate everything greater than $t$ to $maxval$ while keeping other intact:\n",
    "$$\n",
    "I_{trunc}(x,y) = \n",
    "\\begin{cases}\n",
    "  I(x,y) & \\textit{if } I(x,y) \\leqslant t,  \\\\\n",
    "  maxval &  \\textit{if } I(x,y)> t,\n",
    "\\end{cases}\n",
    "$$\n",
    "- `cv2.THRESH_TOZERO` - threshold everything less or equal than $t$ to $0$ while keeping other intact:\n",
    "$$\n",
    "I_{tozero}(x,y) = \n",
    "\\begin{cases}\n",
    "  0 & \\textit{if } I(x,y) \\leqslant t,  \\\\\n",
    "  I(x,y) &  \\textit{if } I(x,y)> t,\n",
    "\\end{cases}\n",
    "$$\n",
    "- `cv2.THRESH_TOZERO_INV` - threshold everything grater then $t$ to $0$ while keeping other intact:\n",
    "$$\n",
    "I_{tozero}(x,y) = \n",
    "\\begin{cases}\n",
    "  I(x,y) & \\textit{if } I(x,y) \\leqslant t,  \\\\\n",
    "  0 &  \\textit{if } I(x,y)> t,\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Simple threshold binarization according to the formula above is executed by using the `cv2.THRESH_BINARY`  method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try executing the single thresholding with a threshold value of $127$ to segment every pixel above the threshold to $1$ and every pixel below the threshold to $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single thresholding\n",
    "t = 127\n",
    "ret, I1st = cv.threshold(I1gray, t, 1, cv.THRESH_BINARY)\n",
    "ShowImages([(\"Single thresholding\", I1st)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By varying the thresholding parameter `t` we can shift the binarization boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Double Thresholding\n",
    "\n",
    "In some cases, the single thresholding is not enough, for example, if we need to select a region. In this case, the double (range) thresholding is used. The double thresholding method is described with the following formula:\n",
    "\n",
    "$$\n",
    "\tI_{dt}(x,y) = \n",
    "\t\\begin{cases}\n",
    "\t\t0, I(x,y) \\leqslant t_1,  \\\\\n",
    "\t\t1, t_1 < I(x,y) \\leqslant t_2,  \\\\\n",
    "\t\t0, I(x,y) > t_2,\n",
    "\t\\end{cases}\n",
    "$$\n",
    "\n",
    "where \n",
    "- $I$ is the source image;\n",
    "- $I_{dt}$ is the double threshold binarized image;\n",
    "- $t_1$ is the lower binarization threshold;\n",
    "- $t_2$ is the upper binarization threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV does not provide a special function for double thresholding, however, it can be done by the subsequent call of two threshold functions with different methods:\n",
    "1. At first, should set all values above $t_2$ to zeros while keeping other intact (`THRESH_TOZERO_INV` method);\n",
    "2. Secondly, should set all values below $t_1$ to zeros (`THRESH_BINARY` method).\n",
    "\n",
    "Let's try executing the double thresholding with $(127, 200)$ range to segment every pixel within this range to $1$ and every pixel outside of this range to $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double thresholding\n",
    "t1 = 127\n",
    "t2 = 200\n",
    "ret, I1dt = cv.threshold(I1gray, t2, 1, cv.THRESH_TOZERO_INV)\n",
    "ret, I1dt = cv.threshold(I1dt, t1, 1, cv.THRESH_BINARY)\n",
    "ShowImages([(\"Double thresholding\", I1dt)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By varying the thresholding parameters `t1` and `t2` we can shift the binarization range borders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1.3 Self-work**\n",
    "\n",
    "> ***Self-work***\n",
    ">\n",
    "> Take some arbitrary image and threshold it with **single and double** thresholding methods. Display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Place your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.2 Automatic Thresholding**\n",
    "\n",
    "Binarization thresholds $t$, $t_1$, and $t_2$ can either be set manually or calculated using special algorithms. In the case of automatic threshold calculation, the following algorithms can be used.\n",
    "\n",
    "### 1.2.1 Mean value\n",
    "\n",
    "1. Find the maximum $I_{max}$ and minimum $I_{min}$ intensity values of the original grayscale image and find their arithmetic mean. The arithmetic mean will be the global binarization threshold $t$:\n",
    "\n",
    "$$\n",
    "  t = \\dfrac{I_{max}-I_{min}}{2}.\n",
    "$$\n",
    "\n",
    "### 1.2.2 Intensity gradient weighting\n",
    "\n",
    "Find the optimal threshold $t$ based on the modulus of each pixel intensity gradient. For this, at first, it is required to calculate the modulus of the gradient at each image point $(x,y)$:\n",
    "\n",
    "$$\n",
    "\tG(x,y) = \\max \\left\\{|I(x+1, y)-I(x-1, y)|, \\\\|I(x,y+1)-I(x,y-1)| \\right\\},\n",
    "$$\n",
    "\n",
    "then calculate the optimal threshold value $t$:\n",
    "\n",
    "$$\n",
    "\tt=\\dfrac{\\sum_{x=0}^{X-1} \\sum_{y=0}^{Y-1} I(x,y)G(x,y)}{\\sum_{x=0}^{X-1} \\sum_{y=0}^{Y-1} G(x,y)}.\n",
    "$$\n",
    "\n",
    "### 1.2.3 Otsu method\n",
    "\n",
    "The optimal threshold $t$ can be calculated by the statistical Otsu method [1] which splits all pixels into two classes $1$ and $2$. This method minimizes the variance within each class $\\sigma_1^2(t)$ and $\\sigma_2^2(t)$ and maximizes the variance between classes.\n",
    "\n",
    "The algorithm for calculating the threshold by the Otsu method consists of the following steps:\n",
    "\n",
    "1. Compute an image histogram of intensities, and probabilities $p_i=\\dfrac{n_i}{N}$ for each intensity level, where $n_i$ is the number of pixels with intensity level $i$, and $N$ is the number of pixels in the image.\n",
    "2. Set the initial threshold $t=0$ and threshold $k \\in (0, L)$, which divides all pixels into two classes, where $L$ is the maximum value of the image intensity.\n",
    "3. In the loop for each value of threshold from $k=1$ to $k=L-1$:\n",
    "    1. Compute probabilities of two classes $\\omega_j(0)$, and arithmetic mean $\\mu_j(0)$, where $j=\\overline{1,2}$:\n",
    "\n",
    "    $$\n",
    "    \\omega_1(k)=\\sum_{s=0}^{k}p_s,\n",
    "    $$\n",
    "    $$\n",
    "    \\omega_2(k)=\\sum_{s=k+1}^{L}p_s=1-\\omega_1(k),\n",
    "    $$\n",
    "    $$\n",
    "\t\t\t\\mu_1(k)=\\sum_{s=0}^{k} \\dfrac{s \\cdot p_s}{\\omega_1},\n",
    "    $$\n",
    "    $$\n",
    "\t\t\t\\mu_2(k)=\\sum_{s=k+1}^{L} \\dfrac{s \\cdot p_s}{\\omega_2}.\n",
    "    $$\n",
    "\t  2. Calculate the interclass variance $\\sigma_b^2(k)$:\n",
    "    $$\n",
    "    \\sigma_b^2(k)=\\omega_1(k)\\omega_2(k)(\\mu_1(k)-\\mu_2(k))^2.\n",
    "    $$\n",
    "\t  3. If the calculated value $\\sigma_b^2(k)$ is greater than the current value $t$, then assign the value of the interclass variance to the threshold $t=\\sigma_b^2(k)$.\n",
    "4. The optimal threshold $t$ corresponds to the maximum $\\sigma_b^2(k)$.\n",
    "\n",
    "### References\n",
    "1. Otsu, N. (1979). A threshold selection method from gray-level histograms. IEEE Transactions on Systems, Man, and Cybernetics, 9 (1), 62â€“66. https://doi.org/10.1109/TSMC.1979.4310076"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In OpenCV thresholding with the threshold calculated by the Otsu method is executed with the same `cv2.threshold(...)` function. To use the Otsu threshold method you have to use the `THRESH_OTSU` parameter as a thresholding method. This function can be used to calculate the Otsu threshold without thresholding an image as well, for this we have to run it in a dry run mode:\n",
    "\n",
    "```python\n",
    "tr, ret = cv2.threshold(I, 0, 1, cv2.THRESH_OTSU + cv2.THRESH_DRYRUN)\n",
    "print(\"The Otsu threshold value is {}\".format(tr))\n",
    "```\n",
    "\n",
    "Let's try executing the thresholding with the threshold calculated by the Otsu method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otsu thresholding\n",
    "tr, I1otsu = cv.threshold(I1gray, 0, 1, cv.THRESH_OTSU)\n",
    "ShowImages([(\"Otsu thresholding\", I1otsu)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Adaptive thresholding\n",
    "\n",
    "Adaptive methods do not work with the entire image, but only with its fragments. Such approaches are often used when working with images that represent non-uniformly illuminated objects.\n",
    "\n",
    "In OpenCV adaptive thresholding  is performed with the `cv2.adaptiveThreshold(image, maxval, algorithm, method, blockSize, c)` function. It supports two adaptive thresholding algorithms:\n",
    "- `ADAPTIVE_THRESH_MEAN_C` uses simple rectangular kernel mean;\n",
    "- `ADAPTIVE_THRESH_GAUSSIAN_C` uses a kernel with Gauss weights. \n",
    "\n",
    "These algorithms support two thresholding methods:\n",
    "- `cv2.THRESH_BINARY` to set everything greater than the threshold value to the $maxval$ and everything else to $0$;\n",
    "- `cv2.THRESH_BINARY_INV` to set everything greater than the threshold value to the $0$ and everything else to $maxval$.\n",
    "\n",
    "Kernel size is defined by the `blockSize` parameter, and the parameter value `c` is subtracted from the mean value calculated within a window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I1ta = cv.adaptiveThreshold(I1gray, 255, cv.ADAPTIVE_THRESH_GAUSSIAN_C, cv.THRESH_BINARY, 121, 5)\n",
    "ShowImages([(\"Adaptive thresholding\", I1ta)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2.5 Self-work**\n",
    "\n",
    "> ***Self-work***\n",
    ">\n",
    "> Take some arbitrary image and threshold it with **four listed** automatic threshold calculation method:\n",
    "> - Mean value threshold\n",
    "> - Intensity gradient weighted threshold\n",
    "> - Otsu threshold\n",
    "> - Adaptive threshold\n",
    ">\n",
    "> Display the resulting images.\n",
    ">\n",
    "> ***Notes:***\n",
    "> 1. *To calculate the mean value fast you should use existing NumPy functions.*\n",
    "> 2. *To make gradient calculation faster you may consider using the `cv2.copyMakeBorder(image, 1, 1, 1, 1, cv2.BORDER_REPLICATE)` function to create a copy of an image with 1 pixel border that replicates the border pixel value to extension region. Then you can use this image with a 1-pixel border to calculate gradients fast with NumPy functions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Place your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Task 2. Color Segmentation**\n",
    "\n",
    "***Take an arbitrary image containing the face(s). Perform the image segmentation according to the Weber principle. Perform the image segmentation based on the skin color and try different formulas on photos with various photo illumination conditions.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider several basic methods of image segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1 Weber principle**\n",
    "\n",
    "The *Weber principle* tells us that there is a not noticeable distance for the human eye and depending on the intensity value, this range can be calculated using the formula:\n",
    "\n",
    "$$\n",
    "\tW(I) = \n",
    "\t\\begin{cases}\n",
    "\t\t20-\\dfrac{12I}{88} & \\textit{if } 0 \\leqslant I \\leqslant 88,  \\\\\n",
    "\t\t0,002(I-88)^2 & \\textit{if } 88 < I \\leqslant 138,  \\\\\n",
    "\t\t\\dfrac{7(I-138)}{117}+13 & \\textit{if } 138 < I \\leqslant 255.\n",
    "\t\\end{cases}\n",
    "$$\n",
    "\n",
    " So, the *Weber principle* assumes that the human eye does not perceive well the difference in gray levels between $I(n)$ and $I(n)+W(I(n))$, where $W(I(n))$ is the Weber function, $n$ is the class number, $I$ is the piecewise non-linear grayscale function. Based on this principle the segmentation algorithm is designed for the segmentation of grayscale images.\n",
    "\n",
    "The Webber principle segmentation algorithm consists of the following steps:\n",
    "\n",
    "1. Initializate initial conditions: first class number $n=1$, all image pixels are not segmented.\n",
    "2. Find the minimum value of the not-segmented image part and store it to $I(n)$.\n",
    "3. Calculate the $W(I(n))$ value according to the Weber formula and assign the segment $n$ and the value $I(n)$ to all pixels whose intensities are in the range $[I(n),I(n)+W(I(n))]$.\n",
    "4. If there are any not-segmented pixels in the image, then increment $n$ ($n=n+1$) and go to step 2. \n",
    "5. If there are no not segmented pixels then the segmentation is finished.\n",
    "\n",
    "The segmentation data can be displayed with artificial colors and a JET color scheme:\n",
    "\n",
    "```python\n",
    "Ijet = cv.applyColorMap(((I - I.min()).astype(np.float32) * 255 / \n",
    "                         (I.max() - I.min())).astype(np.uint8), cv.COLORMAP_JET)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1.1 Self-work**\n",
    "\n",
    "> ***Self-work***\n",
    ">\n",
    "> Implement the image segmentation algorithm based on the Weber principle. Display the resulting images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Place your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2 Skin Color Segmentation**\n",
    "\n",
    "The general principle of skin color segmentation is determining the criterion for the proximity of the pixel intensity to the skin tone. It is quite difficult to describe the *skin tone* analytically since its description is based on the human perception of color. Moreover, it changes with lighting, differs among different nationalities, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several analytical descriptions for images in the RGB color space that allow a pixel to be assigned to the *skin* class of the following conditions. For example, at uniform daylight illumination, the following conditions may be used to segment a part of an image as skin:\n",
    "$$\n",
    "\t\\begin{cases}\n",
    "\t\tR > 95, \\\\\n",
    "\t\tG > 40, \\\\\n",
    "\t\tB > 20, \\\\\n",
    "\t\t\\max\\{R,G,B\\}-\\min\\{R,G,B\\} > 15, \\\\\n",
    "\t\t|R-G| > 15, \\\\\n",
    "\t\tR > G, \\\\\n",
    "\t\tR > B,\n",
    "\t\\end{cases}\n",
    "$$\n",
    "\n",
    "In the case of the flashlight or daylight lateral illumination:\n",
    "\n",
    "$$\n",
    "\t\\begin{cases}\n",
    "\t\tR > 220, \\\\\n",
    "\t\tG > 210, \\\\\n",
    "\t\tB > 170, \\\\\n",
    "\t\t|R-G| \\leqslant 15, \\\\\n",
    "\t\tG > B, \\\\\n",
    "\t\tR > B,\n",
    "\t\\end{cases}\n",
    "$$\n",
    "\n",
    "Also, there is a formula that is based on normalized RGB values ($r, g, b$ which are normalized by the sum of $R, G, B$):\n",
    "\n",
    "$$\n",
    "\t\\begin{cases}\n",
    "\t\tr=\\frac{R}{R+G+B}, \\\\\n",
    "\t\tg=\\frac{G}{R+G+B}, \\\\\n",
    "\t\tb=\\frac{B}{R+G+B}, \\\\\n",
    "\t\t\\frac{r}{g}>1,185, \\\\\n",
    "\t\t\\frac{rb}{(r+g+b)^2}>0,107, \\\\\n",
    "\t\t\\frac{rg}{(r+g+b)^2}>0,112.\n",
    "\t\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement these segmentation methods with OpenCV we should use NumPy matrix operations and create a mask to segment skin. Let us implement the skin color segmentation for the first of these three formulas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will load a random image with a face on it\n",
    "# (image source: https://thispersondoesnotexist.com/)\n",
    "import requests\n",
    "\n",
    "url = \"https://thispersondoesnotexist.com/\"\n",
    "try:\n",
    "  response = requests.get(url)\n",
    "  I2 = cv.imdecode(np.asarray(bytearray(response.content), dtype = np.uint8), cv.IMREAD_COLOR)\n",
    "  if not isinstance(I2, np.ndarray) or I2.data == None:\n",
    "    print(\"Error loading URL \\\"{}\\\"\".format(url))\n",
    "except BaseException:\n",
    "  # If something got wrong then load a local failsafe file\n",
    "  print(\"Something went wrong while loading URL \\\"{}\\\"\".format(url))\n",
    "  print(\"Loading a local failsafe file...\")\n",
    "\n",
    "  fn = \"images/people.jpg\"\n",
    "  I2 = cv.imread(fn, cv.IMREAD_COLOR)\n",
    "  if not isinstance(I2, np.ndarray) or I2.data == None:\n",
    "    print(\"Error reading file \\\"{}\\\"\".format(fn))\n",
    "    exit()\n",
    "\n",
    "# Split into layers\n",
    "I2BGR = cv.split(I2)\n",
    "\n",
    "# Build up the mask\n",
    "I2skin = np.logical_and.reduce(\n",
    "          [I2BGR[2] > 95, # R > 95\n",
    "            I2BGR[1] > 40, # G > 40\n",
    "            I2BGR[0] > 20, # B > 20\n",
    "            np.maximum.reduce(I2BGR) - np.minimum.reduce(I2BGR) > 15, # Max(R,G,B) - Min(R,G,B) > 15\n",
    "            np.abs(I2BGR[2] - I2BGR[1]) > 15, # |R - G| > 15\n",
    "            I2BGR[2] > I2BGR[0],              # R > B\n",
    "            I2BGR[2] > I2BGR[1]])             # G > B\n",
    "\n",
    "# Create a copy of the source image and remove everything except for the face from it\n",
    "I2copy = I2.copy()\n",
    "I2copy[~I2skin] = 0\n",
    "\n",
    "# Display the result\n",
    "ShowImages([(\"Source image\", I2), \n",
    "            (\"Skin segment\", I2skin),\n",
    "            (\"Skin only\", I2copy)], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2.1 Self-work**\n",
    "\n",
    "> ***Self-work***\n",
    ">\n",
    "> Implement the **two** remaining skin segmentation algorithms. Display the resulting images.\n",
    ">\n",
    "> ***Notes.***\n",
    "> 1. *Formula 2 works well only for direct flashlight illumination, so it would segment nothing on most of the regular images. You may use \"images/flashlight.jpg\" image to check your result.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Place your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Task 3. Chromatic Segmentation**\n",
    "\n",
    "***Take an arbitrary image containing a limited number of colored objects. Perform image segmentation in the uniform color space by the nearest neighbors method. Perform image segmentation in the CIE Lab color space by the $k$-means method .***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the CIE Lab color space, the value of lightness is separated from the value of the chromatic components of the color (hue, saturation). The lightness of a color is given by the $L$ coordinate, which can range from $0$ (dark) to $100$ (light). The chromatic component of a color is given by two Cartesian coordinates $a$ (which defines the color position in the range from *green* $(-128)$ to *red* $(127)$) and $b$ (which defines the color position in the range from *blue* $(-128)$ to *yellow* $(127)$). In CIE Lab color space the grayscale colored pixels of an image have zero $a$ and $b$ coordinates. We can use the CIE Lab color space to perform the segmentation of an image based only on chromatic color components and ignore lightness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1 Segmentation in CIE Lab color space**\n",
    "\n",
    "The algorithm's general idea is to divide a color image into segments of dominant colors. For this, we will let the user select a set of dominant colors, then search for a distance to each of these colors and segment image pixels by the shortest distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, we have to let the user select points on the image. To do it we will use the OpenCV GUI library and create a window with the mouse callback function `MouseHandler()`. This function will register each left mouse button click and add corresponding pixel coordinates to an array with selected points. This will continue until the window is shown or the user presses an ESC button.\n",
    "\n",
    "So, the general workflow is as follows:\n",
    "1. Create a GUI window with `cv2.imhow(window_name, I)` function.\n",
    "2. Register a mouse callback with `cv2.setMouseCallback(window_name, MouseHandler, params)`. \n",
    "3. Run the infinite loop and break it in case if:\n",
    "    - Window is closed (checked by `cv2.getWindowProperty(window_name, cv2.WND_PROP_VISIBLE) < 1`);\n",
    "    - Or the ESC button is pressed (checked by `cv.waitKey(100) == 27`).\n",
    "\n",
    "In the mouse handler (`MouseHandler()`) function we check the event type, append click coordinates to the selected points list, and update the shown image by making a new image and calling  `cv2.imhow(window_name, I)` function again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read an image from file\n",
    "fn = \"images/people.jpg\"\n",
    "I3 = cv.imread(fn, cv.IMREAD_COLOR)\n",
    "if not isinstance(I3, np.ndarray) or I3.data == None:\n",
    "  print(\"Error reading file \\\"{}\\\"\".format(fn))\n",
    "  exit()\n",
    "\n",
    "# Set window name and other parameters\n",
    "window_name = \"Select points\"\n",
    "points = []\n",
    "radius = 10\n",
    "\n",
    "# Define the mouse handler\n",
    "def MouseHandler(event, x, y, flags, param):\n",
    "  # Only the left button click event is processed\n",
    "  # All others are ignored\n",
    "  if event != cv.EVENT_LBUTTONDOWN:\n",
    "    return\n",
    "  # Append new point coordinates\n",
    "  points.append((x, y))\n",
    "  # Create a new image \n",
    "  I3copy = I3.copy()\n",
    "  # And draw circles for each registered mouse click\n",
    "  for p in points:\n",
    "    cv.circle(I3copy, p, radius, (0, 0, 255), 2)\n",
    "  # Then update the image shown in the window\n",
    "  cv.imshow(window_name, I3copy)\n",
    "\n",
    "# Create a window, show it, and register the callback\n",
    "cv.namedWindow(window_name)\n",
    "cv.imshow(window_name, I3)\n",
    "cv.setMouseCallback(window_name, MouseHandler)\n",
    "\n",
    "# Wait for an ESC key press or window is closed\n",
    "while True:\n",
    "  if cv.waitKey(100) == 27:\n",
    "    break\n",
    "  if cv.getWindowProperty(window_name, cv.WND_PROP_VISIBLE) < 1:\n",
    "    break\n",
    "\n",
    "# Destroy all windows data\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "# Draw circles for each registered mouse click\n",
    "I3copy = I3.copy()\n",
    "for p in points:\n",
    "  cv.circle(I3copy, p, radius, (0, 0, 255), 2)\n",
    "# And display it\n",
    "ShowImages([(\"Labels\", I3copy)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the list of the selected points we may proceed with the general segmentation algorithm:\n",
    "1. Convert an image to CIE Lab color space.\n",
    "2. Take an area around selected pixels and calculate the mean value in this area to get the reference $ab$ color for each segment.\n",
    "3. Calculate the Euclidean distance in $ab$ layers ($d = \\sqrt{(a_2 - a_1)^2 + (b_2 - b_1)^2})$ for each image pixel from the selected points.\n",
    "4. For each image pixel select the nearest color in $ab$ coordinate space. This will be the desired segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert to CIE Lab\n",
    "I3lab = cv.cvtColor(I3, cv.COLOR_BGR2LAB)\n",
    "# Split it into layers\n",
    "I3lab = cv.split(I3lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Instead of taking the selected pixel value\n",
    "# We will calculate the mean color value in an area around the pixel\n",
    "points_ab = []\n",
    "points_bgr = []\n",
    "for p in points:\n",
    "  mask = np.zeros_like(I3lab[0])\n",
    "  cv.circle(mask, p, radius, 255, -1)\n",
    "  a = I3lab[1].mean(where = mask > 0)\n",
    "  b = I3lab[2].mean(where = mask > 0)\n",
    "  points_ab.append((a, b))\n",
    "  points_bgr.append(I3[mask > 0, :].mean(axis=(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Calculate distance\n",
    "distances = []\n",
    "for ab in points_ab:\n",
    "  distances.append(np.sqrt(\n",
    "    np.power(I3lab[1] - ab[0], 2) + \n",
    "    np.power(I3lab[2] - ab[1], 2)))\n",
    "  \n",
    "# Calculate the minimum distance among them all\n",
    "distance_min = np.minimum.reduce(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create segmented areas\n",
    "I3labels = np.zeros_like(I3lab[0], dtype = np.uint8)\n",
    "\n",
    "I3segments = []\n",
    "# Fill them for each selected color\n",
    "for i in range(len(points_ab)):\n",
    "  Itmp = np.zeros_like(I3)\n",
    "  # Segmentation mask is created by finding all pixels with a matching distance\n",
    "  mask = distance_min == distances[i]\n",
    "  # Add labels to the segmentation plot\n",
    "  I3labels[mask] = i\n",
    "  # Segmented area of source image\n",
    "  Itmp[mask] = I3[mask]\n",
    "  # Add the segmented area to a list of segments\n",
    "  I3segments.append((\"Segment {}\".format(i), Itmp))\n",
    "\n",
    "# Display it\n",
    "ShowImages([(\"Labels\", cv.applyColorMap(((I3labels - I3labels.min()).astype(np.float32)  * 255 / \n",
    "                                         (I3labels.max() - I3labels.min())).astype(np.uint8), cv.COLORMAP_JET))])\n",
    "ShowImages(I3segments, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can place the distribution of image pixel colors to a plot in the $(a, b)$ coordinate system. The plot pixel color is defined by a mean $BGR$ which was calculated along with calculating the mean $ab$ value and stored in the `points_bgr` array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The color distribution plot\n",
    "I3plot = np.full((256, 256, 3), 255, dtype = np.uint8)\n",
    "for i in range(len(points_ab)):\n",
    "  mask = I3labels == i\n",
    "  I3plot[I3lab[1][mask], I3lab[2][mask], :] = points_bgr[i]\n",
    "\n",
    "# Display it\n",
    "ShowImages([(\"Color plot\", I3plot)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1.1 Self-work**\n",
    "\n",
    "> ***Self-work***\n",
    "> \n",
    "> Try modifying the above algorithm to use the HSV color model.\n",
    ">\n",
    "> ***Notes.***\n",
    "> 1. To convert from BGR to HSV color space you should use the `cv2.COLOR_RGB2HSV` color model conversion parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Place your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2 $k$-means Clustering**\n",
    "\n",
    "The idea of the method is to determine the centers of $k$-clusters and assign to each cluster the pixels closest to these centers. All pixels are considered as vectors $x_i, i=\\overline{1,p}$. The segmentation algorithm consists of the following steps:\n",
    "1. Randomly determine $k$ vectors $m_j, j=\\overline{1,k}$, which are declared as initial centers of clusters.\n",
    "2. Update mean values of the vectors $m_j$ by calculating distances from each vector $x_i$ to each $m_j$ and their classification according to the criterion of minimal distance from the vector to the cluster, recalculation of average values $m_j$ across all clusters.\n",
    "3. Repeat step $2$ until the cluster centers stop changing.\n",
    "\n",
    "The implementation of the method is very similar to the previous approach and contains a number of similar actions. We will work in the CIE Lab color space, so the first step is transformation from the BGR space to the Lab and splitting into layers:\n",
    "\n",
    "```python\n",
    "Ilab = cv2.cvtColor(I, cv2.COLOR_BGR2LAB)\n",
    "Ilab = cv2.split(Ilab)\n",
    "```\n",
    "\n",
    "Then we have to merge $a$ and $b$ layers of the Lab representation of our source image and use the NumPy reshaping function to create the two-dimensional array of the image pixel colors:\n",
    "\n",
    "```python\n",
    "ab = cv2.merge([Ilab[1], Ilab[2]])\n",
    "ab = ab.reshape(-1, 2).astype(np.float32)\n",
    "```\n",
    "\n",
    "This two-dimensional array can be then passed to the `cv2.kmeans()` function that performs the $k$-means clustering. Parameters allow us to define stop criteria and the number of attempts to select a set of starting points. In the following code stop criteria is defined as not more than 10 iterations of difference between steps less than 1. Starting points are selected randomly (due to the `cv2.KMEANS_RANDOM_CENTERS` flag being used) and selection is done 10 times. After algorithm execution is finished, the returned `labels` parameter is reshaped back to an original image shape:\n",
    "\n",
    "```python\n",
    "k = 3\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "ret, labels, centers = cv2.kmeans(ab, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "labels = labels.reshape((Ilab[0].shape))\n",
    "```\n",
    "\n",
    "Then, it is possible to use the generated labels to segment the image into a set of images or masks:\n",
    "\n",
    "```python\n",
    "segmentedFrames = []\n",
    "for i in range(k):\n",
    "  Itmp = np.zeros_like(I)\n",
    "  mask = labels == i\n",
    "  Itmp[mask] = I[mask, :]\n",
    "  segmentedFrames.append(Itmp)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2.1 Self-work**\n",
    "\n",
    "> ***Self-work***\n",
    "> \n",
    "> Implement the $k$-means clustering algorithm and display the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Place your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Task 4. Texture Segmentation**\n",
    "\n",
    "***Take an arbitrary image containing two heterogeneous textures. Perform texture segmentation of the image.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In texture segmentation, three main approaches are used to describe texture: statistical, structural, and spectral. In the practical assignment, we will consider a statistical approach that describes the segment texture as smooth, rough, or grainy. \n",
    "\n",
    "We will consider the image intensity $I$ as a random variable $z$, which corresponds to the distribution probability $p(z)$ calculated from the image histogram. The *Central moment* of order $n$ of a random variable $z$ is the parameter $\\mu_n(z)$ calculated by the formula:\n",
    "\n",
    "$$\n",
    "\t\\mu_n(z)=\\sum_{i=0}^{L-1}(z_i-m)^n p(z_i),\n",
    "$$\n",
    "\n",
    "where \n",
    "- $L$ is the number of intensity levels;\n",
    "- $m$ is the mean value of a random variable $z$:\n",
    "\n",
    "$$\n",
    "\tm=\\sum_{i=0}^{L-1} z_i p(z_i).\n",
    "$$\n",
    "\n",
    "The expression above implies that $\\mu_0=1$ and $\\mu_1=0$. To describe the texture, the *variance* of a random variable is important, which is equal to the second moment $\\sigma^2(z)=\\mu_2(z)$ and is a measure of the brightness contrast. It can be used to calculate the features of *smoothness*. \n",
    "\n",
    "So, let us introduce a measure of relative smoothness $R$:\n",
    "\n",
    "$$\n",
    "\tR=1-\\dfrac{1}{1+\\sigma^2(z)},\n",
    "$$\n",
    "\n",
    "The relative smoothness is zero for areas with constant intensity (zero variance) and approaches unity for large variances $\\sigma^2(z)$. For grayscale images with an intensity range $[0, 255]$, it is necessary to normalize the variance to the range $[0, 1]$, since the values of the variances will be too large for the initial range. Normalization is carried out by dividing the variance $\\sigma^2(z)$ by $(L-1)^2$.\n",
    "\n",
    "The *standard deviation* is also often used as a texture characteristic:\n",
    "\n",
    "$$\n",
    "\ts=\\sigma(z).\n",
    "$$\n",
    "\n",
    "The third moment is the *histogram symmetry characteristic*. To estimate texture features, the *entropy* $E$ function is used, which determines the spread of neighboring pixels intensities:\n",
    "\n",
    "$$\n",
    "\tE=-\\sum_{i=0}^{L-1} p(z_i) \\log_2{p(z_i)}.\n",
    "$$\n",
    "\n",
    "Another important characteristic that describes the texture is the *uniformity measure* $U$, which evaluates the uniformity of the histogram:\n",
    "\n",
    "$$\n",
    "\tU=\\sum_{i=0}^{L-1} p^2(z_i).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try doing segmentation using *Entropy* of a pixel neighborhood as a texture characteristic. For this we have to calculate the image Entropy first, then threshold it to get the starting morphology. Unfortunately, OpenCV does not provide the corresponding function for Entropy calculation, however, in the case of Python it can be found in the scikit-image library named `skimage.filters.rank.entropy()`. To define the neighboring area we will use the rectangular $9\\times9$ kernel created by `skimage.morphology.square()` function. Since scikit-image converts an image to `float64` type we have to do a backward conversion along with normalization to a $[0, 1]$ range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read an image from file\n",
    "fn = \"images/texture.jpg\"\n",
    "I4 = cv.imread(fn, cv.IMREAD_COLOR)\n",
    "if not isinstance(I4, np.ndarray) or I4.data == None:\n",
    "  print(\"Error reading file \\\"{}\\\"\".format(fn))\n",
    "  exit()\n",
    "\n",
    "# Convert to grayscale\n",
    "I4gray = cv.cvtColor(I4, cv.COLOR_BGR2GRAY)\n",
    "# And display\n",
    "ShowImages([(\"Source\", I4), (\"Grayscale\", I4gray)], 2)\n",
    "\n",
    "# Calculate entropy\n",
    "I4e = skimage.filters.rank.entropy(I4gray, skimage.morphology.square(9)).astype(np.float32)\n",
    "# Normalize it to [0, 1] range\n",
    "I4en = (I4e - I4e.min()) / (I4e.max() - I4e.min())\n",
    "# And display\n",
    "ShowImages([(\"Entropy\", I4en)], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to threshold the entropy image and filter the resulting morphology. For thresholding, we will use the Otsu thresholding, while the morphological filtering will be performed in three steps: \n",
    "1. At first, remove connected regions (equivalent to MATLAB's `bwareaopen()`  function);\n",
    "2. Secondly, remove internal defects with closing operation (executed by `cv2.morphologyEx()` function with `cv2.MORPH_CLOSE` parameter) and rectangular structure element of size $9\\times9$ (created by `cv2.getStructuringElement()` function with shape parameter  `cv2.MORPH_RECT`);\n",
    "3. And, thirdly, fill remaining *holes* (equivalent to MATLAB's `imfill('holes')`  function).\n",
    "\n",
    "Even if OpenCV lacks MATLAB's `bwareaopen(A, dim)` and `imfill(I, 'holes')` functions, they can be easily implemented using OpenCV's `connectedComponentsWithStats()`. You may check the `pa_utils.py` file for the implementation of these functions with OpenCV. However, now we will simply import and use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pa_utils import bwareaopen, imfillholes\n",
    "\n",
    "# Threshold\n",
    "ret, I4t = cv.threshold(np.uint8(I4en * 255), 0, 255, cv.THRESH_OTSU)\n",
    "\n",
    "# Remove connected regions\n",
    "I4bwao = bwareaopen(I4t, 2000)\n",
    "# Remove internal defects with closing\n",
    "nhood = cv.getStructuringElement(cv.MORPH_RECT, (9, 9))\n",
    "I4close = cv.morphologyEx(I4bwao, cv.MORPH_CLOSE, nhood)\n",
    "# Fill holes to get the final mask\n",
    "I4mask = imfillholes(I4close)\n",
    "\n",
    "ShowImages([(\"Threshold\", I4t), (\"bwareaopen\", I4bwao), (\"After closing\", I4close), (\"Fill holes\", I4mask)], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, using the OpenCV `cv2.findContours()` function it's possible to find shape contours and then draw them on a black background with the `cv2.drawContours()` function to define the contour mask. Then we can use this mask to apply the border to the source image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find boundary\n",
    "contours, h = cv.findContours(I4mask, cv.RETR_TREE, cv.CHAIN_APPROX_NONE)\n",
    "I4boundary = np.zeros_like(I4mask)\n",
    "cv.drawContours(I4boundary, contours, -1, 255, 1)\n",
    "# Outline the whole image as well\n",
    "cv.rectangle(I4boundary, (0, 0), (I4.shape[1] - 1, I4.shape[0] - 1), 255)\n",
    "\n",
    "# Separate segments\n",
    "I4seg1 = I4.copy()\n",
    "I4seg1[I4mask == 0] = 0\n",
    "I4seg2 = I4.copy()\n",
    "I4seg2[I4mask != 0] = 0\n",
    "I4split = I4.copy()\n",
    "I4split[I4boundary != 0] = [0, 0, 255]\n",
    "\n",
    "ShowImages([(\"Boundary\", I4boundary), \n",
    "            (\"Split image\", I4split),\n",
    "            (\"Segment 1\", I4seg1),\n",
    "            (\"Segment 2\", I4seg2)], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1 Self-work**\n",
    "\n",
    "> ***Self-work***\n",
    "> \n",
    "> Take an arbitrary image and implement the image texture segmentation. Display the results.\n",
    ">\n",
    "\n",
    "> ***Optional*** (1 extra point)\n",
    ">\n",
    "> Use another statistical texture parameter instead of Entropy when implementing the texture segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Place your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Questions**\n",
    "\n",
    "***Please answer the following questions:***\n",
    "\n",
    " - *When is it appropriate to use Weber segmentation?*\n",
    " > Put your answer here\n",
    " >\n",
    " >\n",
    "\n",
    " - *What are the $a$ and $b$ color coordinates values for a grayscale image in the CIE Lab color space?*\n",
    " > Put your answer here\n",
    " >\n",
    " >\n",
    "\n",
    " - *What is the reason for performing an image segmentation in the CIE Lab color space instead of the original RGB one?*\n",
    " > Put your answer here\n",
    " >\n",
    " >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Conclusion**\n",
    "\n",
    "***What have you learned with this task? Don't forget to conclude it.***\n",
    "\n",
    " > Put your conclusion here\n",
    " >\n",
    " >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
